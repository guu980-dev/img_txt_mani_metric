{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import json\n",
    "from augclip import AugCLIP\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = Path(\"/home/server08/yoonjeon_workspace/augclip_data\")\n",
    "edit_path = work_dir / \"tedbench/edit_prompt.json\"\n",
    "dataset = \"tedbench\"\n",
    "models = [\"imagic\"]\n",
    "augclip = AugCLIP(\"cuda:0\", \"ViT-B/16\")\n",
    "\n",
    "score_path = work_dir / f\"scores/{dataset}/augclip.json\"\n",
    "\n",
    "with open(edit_path, \"r\") as f:\n",
    "    cfg = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = lambda x, y: torch.einsum(\"ij, kj -> ik\", x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = cfg[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = str(case[\"img_name\"]).replace(\"ROOT_DIR\", str(work_dir))\n",
    "manip_path = str(case[\"output_name\"]).replace(\"ROOT_DIR\", str(work_dir))\n",
    "src_image = Image.open(img_path).convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "src_text, tgt_text = case[\"prompt\"]\n",
    "src_text_feat, tgt_text_feat = augclip.get_text_embeddings(src_text), augclip.get_text_embeddings(tgt_text)\n",
    "\n",
    "src_desc, tgt_desc = case[\"source_desc\"], case[\"target_desc\"]\n",
    "with open(\"augclip/desc/trial1/\")\n",
    "src_desc_feat, tgt_desc_feat = augclip.get_text_embeddings(src_desc), augclip.get_text_embeddings(tgt_desc)\n",
    "\n",
    "delta_T = F.normalize(tgt_text_feat - src_text_feat, p=2, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"imagic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "manip_model_path = str(manip_path).replace(\"MODEL_NAME\", model)\n",
    "manip_image = Image.open(manip_model_path).convert(\"RGB\").resize((512, 512))\n",
    "\n",
    "src_img_feat, tgt_img_feat = augclip.get_image_embeddings([src_image]), augclip.get_image_embeddings([manip_image])\n",
    "\n",
    "input_arguments = {\n",
    "    \"src_img_feat\": src_img_feat,\n",
    "    \"tgt_img_feat\": tgt_img_feat,\n",
    "    \"src_text_feat\": src_text_feat,\n",
    "    \"tgt_text_feat\": tgt_text_feat,\n",
    "    \"src_desc_feat\": src_desc_feat,\n",
    "    \"tgt_desc_feat\": tgt_desc_feat,\n",
    "    \"src_set_feat\": src_desc_feat,\n",
    "    \"tgt_set_feat\": tgt_desc_feat,\n",
    "    \"thres\": 0 # probability of target + src attr happening\n",
    "}\n",
    "change_attr = False # if case=='attr' else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type Tensor doesn't define __round__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m inv_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m inv_w\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ordered_desc, inv_w):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(d, \u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndigits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: type Tensor doesn't define __round__ method"
     ]
    }
   ],
   "source": [
    "src_sims = cs(src_desc_feat, src_img_feat)\n",
    "in_source = (src_sims > 0.).flatten() # check if the source image has the given source property\n",
    "src_desc_feat = src_desc_feat[in_source]\n",
    "\n",
    "dist1 = cs(src_desc_feat, src_desc_feat)\n",
    "mask = ~torch.eye(dist1.shape[0], dtype=bool)\n",
    "dist1 = dist1[mask].reshape(dist1.shape[0], -1)\n",
    "\n",
    "dist2 = cs(src_desc_feat, tgt_desc_feat)\n",
    "\n",
    "w1 = dist1.var(dim=-1)\n",
    "ordered_weight, idxs = w1.sort(descending=False)\n",
    "ordered_desc = [src_desc[idx] for idx in idxs]\n",
    "inv_w = 1 / ordered_weight\n",
    "inv_w /= inv_w.max()\n",
    "for d, w in zip(ordered_desc, inv_w):\n",
    "    \n",
    "    print(d, round(w, ndigits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 30])\n",
      "torch.Size([30, 29])\n",
      "['Muscular', 'Adventurous', 'Graceful', 'Athletic', 'Playful', 'Energetic', 'Dynamic', 'Agile', 'Tail up', 'Free-spirited', 'Alert', 'Healthy', 'Paws off the ground', 'In motion', 'Lively', 'Vibrant', 'High energy', 'Determined', 'Happy', 'Action shot', 'Bouncy', 'Excited expression', 'Frolicking', 'Joyful', 'Acrobatic', 'Airborne', 'Focused', 'Mid-air', 'Leaping', 'Jumping']\n"
     ]
    }
   ],
   "source": [
    "dist1 = cs(tgt_desc_feat, src_desc_feat)\n",
    "print(dist1.shape)\n",
    "\n",
    "dist2 = cs(tgt_desc_feat, tgt_desc_feat)\n",
    "mask = ~torch.eye(dist2.shape[0], dtype=bool)\n",
    "dist2 = dist2[mask].reshape(dist2.shape[0], -1)\n",
    "print(dist2.shape)\n",
    "\n",
    "w2 = dist2.var(dim=-1)\n",
    "idxs = w2.sort(descending=False).indices\n",
    "ordered_desc = [tgt_desc[idx] for idx in idxs]\n",
    "print(ordered_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A photo of a jumping dog.\n"
     ]
    }
   ],
   "source": [
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Prompt Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/server08/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import PIL\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# FILL API KEY HERE\n",
    "client = OpenAI()\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\n",
    "def generate_descs(caption, source_text, target_text, prompt_option=0, temperature=0., frequency_penalty=0.):\n",
    "    if prompt_option==0:\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": f\"Give me a python list of 30 visual characteristics \\\n",
    "                    describes {caption}.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"Return python list of that 30 text \\\n",
    "                    descriptions that depict a photo of a fluffy brown coated dog\"\n",
    "            },\n",
    "            {\"role\": \"assistant\", \"content\": \n",
    "                        \"\"\"\n",
    "                        [\n",
    "                            \"Furry coat\",\n",
    "                            \"Four legs\",\n",
    "                            \"Tail wagging\",\n",
    "                            \"Barking\",\n",
    "                            \"Playful behavior\",\n",
    "                            \"Snout\",\n",
    "                            \"Collar\",\n",
    "                            \"Leash\",\n",
    "                            \"Walking on all fours\",\n",
    "                            \"Wagging tail\",\n",
    "                            ]\n",
    "                        \"\"\" \n",
    "            }\n",
    "            ,\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"Give me a python list of 30 visual characteristics \\\n",
    "                    describes {caption}.\"}\n",
    "        ]\n",
    "    \n",
    "    elif prompt_option==1:\n",
    "        question = f\"\"\"\n",
    "            Given the source text and target text, give me a python list of 30 visual characteristics that a target might have. \n",
    "            Note that you should extract the characteristics of only the target compared to the source.\n",
    "            The visual features must be easy to portray: color, texture, shape, material, objects that are seen together, usage and etc.\n",
    "            For example, \n",
    "            1. Source: \"a photo of a dog\", Target: \"a photo of a cat\", answer: [\"Slanted almond-shaped eyes\", \"Soft and fluffy fur coat\", \"Long and elegant whiskers\", \"Pointed ears with tufts of fur\", \"Graceful and agile movements\", ...]\n",
    "            2. Source: \"a photo of a horse\", Target: \"A photo of a breakdancing horse\", answer: [\"Windmill\", \"Twisting body\", \"Fur and ears flowing\", ...]\n",
    "\n",
    "            Source: {source_text}\n",
    "            Target: {target_text}\n",
    "            The answer is:\n",
    "            \"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": question}]\n",
    "            \n",
    "    elif prompt_option==2:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": f\"\"\"\n",
    "                            Provide image level characteristics \n",
    "                            such as color, texture, object category information, \n",
    "                            context of appearance, background \n",
    "                            of a given text that represents the image.\n",
    "                            \"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"\"\"\n",
    "                            Given a sentence, analyze the \n",
    "                            visual characteristics of the image where 'A dog is breakdancing',\n",
    "                            that is not shown in the image of 'A dog is sitting'.\n",
    "                            For example, focus on the how breakdancing is different from sitting only and do not mention dog's appearance.\n",
    "                            Rearrange into a python list format.\n",
    "                            \"\"\"\n",
    "            },\n",
    "            {\"role\": \"assistant\", \n",
    "            \"content\": \n",
    "                        \"\"\"\n",
    "                        [\n",
    "                            \"motion: A dog shows dynamic movement\",\n",
    "                            \"motion: A dog is spinning on its head\", \n",
    "                            \"environment: A dog is dancing on dance floor\",\n",
    "                            \"environment: A dog is dancing in a hip hop scene\",\n",
    "                            \"appearance: A dog shows stylish dancer outfit\", \n",
    "                            \"appearance: A dog has its legs up in the air\",\n",
    "                        ]\n",
    "                        \"\"\" \n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"\"\"\n",
    "                Given a sentence, analyze the \n",
    "                visual characteristics of the image where {target_text},\n",
    "                that is not shown in the image of {source_text}.\n",
    "                Rearrange into a python list format.\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "    completion = client.chat.completions.create(\n",
    "        model='gpt-3.5-turbo', \n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "    )\n",
    "    response = completion.choices[0].message.content\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CelebA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:13<00:00,  5.06s/it]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check permutation of Source / Target directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "perm(tgt_desc_feat - src_desc_feat)\n",
    "\n",
    "\n",
    "1. tgt text 와 비교해서 - 이면 bad direction\n",
    "2. tgt text 와 비교해서 + 이면 good direction\n",
    "3. src img 와 비교해서 차이가 크면 bad direction\n",
    "4. src img 와 비교해서 차이가 작으면 good direction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"tedbench\"\n",
    "data_dir = \"../augclip_data/\"\n",
    "freq_pen = 0.\n",
    "prompt_option = 2\n",
    "temp = 0.\n",
    "device = \"cuda:0\"\n",
    "trial = \"trial5\"\n",
    "model = \"kosmos\"\n",
    "\n",
    "os.makedirs(f\"{trial}/\", exist_ok=True)\n",
    "\n",
    "print(dataset)\n",
    "edit_data= read_json(f\"{data_dir}{dataset}/edit_prompt.json\")\n",
    "if dataset == \"dreambooth\":\n",
    "    all_images = set([(item['img_name'], (item['prompt'][0], '')) for item in edit_data])\n",
    "    edit_data = [{'img_name': path[0], 'prompt': path[1], 'output_name': path[0]} for path in all_images]\n",
    "\n",
    "desc_path = f\"{trial}/{dataset}_src.json\"\n",
    "fail_path = f\"{trial}/{dataset}_src_failed.json\"\n",
    "descs = read_json_or_dict(desc_path)\n",
    "failed = read_json_or_dict(fail_path)\n",
    "\n",
    "if model==\"blip\":\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "elif model==\"kosmos\":\n",
    "    processor = AutoProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n",
    "    model = AutoModelForVision2Seq.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n",
    "\n",
    "\n",
    "prompt_dict = {\n",
    "    \"CelebA\": (lambda src_prompt: f\"<grounding>An image has facial features like {src_prompt}, \"),\n",
    "    \"sameswap\": (lambda src_prompt: f\"<grounding>{src_prompt} in the image has visual features like \"),\n",
    "    \"tedbench\": (lambda src_obj: f\"<grounding>An image of {src_obj} has visual features like \"),\n",
    "}\n",
    "\n",
    "descs = {}\n",
    "for item in tqdm(edit_data):\n",
    "    output_name = item[\"output_name\"].split('/')[-1]\n",
    "    if (output_name in descs) or (output_name in failed):\n",
    "        continue\n",
    "    src_prompt, tgt_prompt = item[\"prompt\"]\n",
    "    img_path = item[\"img_name\"].replace('ROOT_DIR/', data_dir)\n",
    "    image = PIL.Image.open(img_path)\n",
    "    if dataset==\"tedbench\":\n",
    "        inp = item[\"object\"][0]\n",
    "    else:\n",
    "        inp = item[\"prompt\"][0]\n",
    "\n",
    "    prompt = prompt_dict[dataset](inp)\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        image_embeds=None,\n",
    "        image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n",
    "        use_cache=True,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Specify `cleanup_and_extract=False` in order to see the raw model generation.\n",
    "    # processed_text1 = processor.post_process_generation(generated_text, cleanup_and_extract=False)\n",
    "    caption, entities = processor.post_process_generation(generated_text)\n",
    "    messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": f\"\"\"\n",
    "                            Extract all descriptions on the given entity in the sentence.\n",
    "                            \"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"\"\"\n",
    "                            Given a sentence \n",
    "                            'red haired girl in the image has visual features like curly hair, wearing a hat and blue pajamas and holding a cup of coffee, \n",
    "                            while a black haired woman is wearing black suit and writing down notes. They are both in a cafe, sitting around a round table'\n",
    "                            Extract all descriptions on the entity 'red haired girl'.\n",
    "                            Then given the descsriptions, augment the given descriptions into a sub-categories of visual characteristics.\n",
    "                            \"\"\"\n",
    "            },\n",
    "            {\"role\": \"assistant\", \n",
    "            \"content\": \n",
    "                        \"\"\"\n",
    "                        descs: [\n",
    "                            \"Red haired girl has curly hair\",\n",
    "                            \"Red haired girl is wearing a hat\", \n",
    "                            \"Red haired girl is wearing blue pajamas\", \n",
    "                            \"Red haired girl is holding a cup of coffee\",\n",
    "                            \"Red haired girl is in a cafe\",\n",
    "                            \"Red haired girl is sitting around a round table\"\n",
    "                        ]\n",
    "                        \"\"\" \n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": f\"\"\"\n",
    "                            Given a sentence \n",
    "                            '{caption}'\n",
    "                            Extract all descriptions on the entity '{inp}'.\n",
    "\n",
    "                            \"\"\"\n",
    "            }\n",
    "        ]\n",
    "    # print(f\"caption: {caption} / source prompt: {src_prompt} / target prompt: {tgt_prompt} / entities: {entities}\")\n",
    "    # descs[\"caption\"] = caption\n",
    "    completion = client.chat.completions.create(\n",
    "        model='gpt-3.5-turbo', \n",
    "        messages=messages,\n",
    "        temperature=temp,\n",
    "        frequency_penalty=freq_pen,\n",
    "    )\n",
    "    response = completion.choices[0].message.content\n",
    "    x = str(response)\n",
    "    s_idx, e_idx = x.find(\"[\"), x.find(\"]\")\n",
    "    desc_list = x[s_idx+1: e_idx].split(\",\")\n",
    "    desc_list = [desc.strip()[1:-1] for desc in desc_list if len(desc.strip()[1:-1])]\n",
    "    processed_desc_list = [desc.split(\":\")[-1].strip() for desc in desc_list]\n",
    "    descs[output_name] = processed_desc_list\n",
    "    # desc = generate_descs(caption, src_prompt, tgt_prompt, entities, temp, freq_pen)\n",
    "    # descs[output_name] = eval(desc.strip('.'))\n",
    "write_json(\"./desc/trial5/sameswap_src.json\", descs)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
